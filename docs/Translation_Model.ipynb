{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c868a4d3-90f5-401b-bbf1-67d27238f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keejuan/Documents/ML/CNENTranslation/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import jieba\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1281c57-7449-4347-a5aa-1072d2e88362",
   "metadata": {},
   "source": [
    "Preprocess data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af05159e-07a8-46c7-84d8-6ee3315919ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        # replace '\\n' with ' '\n",
    "        data = pd.Series(f.readlines())\n",
    "        data = data.str.replace('\\n', ' ')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b1e446-4061-4274-bb52-049fdbedf05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ZH = load_data('chinese.zh')\n",
    "data_EN = load_data('english.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47abd0a9-1bec-489b-8336-76e397c1e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ZH = data_ZH[:10000]\n",
    "data_EN = data_EN[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3a49339-6045-467a-921c-8cab930caf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/s8/pbv_qcy127g2gt_nwhfbrnxr0000gn/T/jieba.cache\n",
      "Loading model cost 0.339 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                    1929 年 还是 1989 年 ?  \n",
       "1       巴黎 - 随着 经济危机 不断 加深 和 蔓延 ， 整个 世界 一直 在 寻找 历史 上 的...\n",
       "2       一 开始 ， 很多 人 把 这次 危机 比作 1982 年 或 1973 年 所 发生 的 ...\n",
       "3       如今 人们 的 心情 却是 沉重 多 了 ， 许多 人 开始 把 这次 危机 与 1929 ...\n",
       "4       目前 的 趋势 是 ， 要么 是 过度 的 克制 （ 欧洲 ） ， 要么 是 努力 的 扩展...\n",
       "                              ...                        \n",
       "9995    根据 像 约翰 · 麦凯恩 这样 的 人 的 观点 ， 最 根本 的 原因 是 华尔街 的 ...\n",
       "9996    虽然 我 不 否认 这样 的 基础 动力 的 存在 ， 但是 ， 我 还是 坚持 认为 这场...\n",
       "9997                              在 美国 ， 有 两个 关键 的 决定 。  \n",
       "9998    第一个 是 在 20 世纪 70 年代 解除 了 对 支付 给 股票 经纪人 的 佣金 的 ...\n",
       "9999    第二个 是 在 20 世纪 90 年代 废止 了 《 格拉斯 - 斯蒂格 尔法 》 对 商业...\n",
       "Length: 10000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cut_ZH(data):\n",
    "    data_cut = data.apply(lambda x: ' '.join(jieba.cut(x)))\n",
    "    return data_cut\n",
    "\n",
    "data_ZH = cut_ZH(data_ZH)\n",
    "data_ZH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ab7b9c-f817-4b83-96ed-8256275737a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ZH = data_ZH.to_frame()\n",
    "df_EN = data_EN.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d2844f-0a1a-4098-9676-343d6c0473af",
   "metadata": {},
   "source": [
    "Constants and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e6c66e-c3a7-4979-84b0-f0bf28a631bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH = 'en'\n",
    "CHINESE = 'zh'\n",
    "PREFIX = \"\"\n",
    "MAX_INPUT_LENGTH = 128\n",
    "MAX_TARGET_LENGTH = 128\n",
    "INPUT_IDS = \"input_ids\"\n",
    "LABELS = \"labels\"\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ad23d28-5d74-4a1a-b68c-486571dd0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds: list, labels: list) -> tuple:\n",
    "    \"\"\"Performs post processing on the prediction text and labels\"\"\"\n",
    "\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7df749b8-468a-4724-ac47-9d1c7d656357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_for_model_fine_tuning(source_lang: list, target_lang: list) -> list:\n",
    "    \"\"\"Takes the input data lists and converts into translation list of dicts\"\"\"\n",
    "\n",
    "    data_dict = dict()\n",
    "    data_dict['translation'] = []\n",
    "\n",
    "    for sr_text, tr_text in zip(source_lang, target_lang):\n",
    "        temp_dict = dict()\n",
    "        temp_dict[CHINESE] = sr_text\n",
    "        temp_dict[ENGLISH] = tr_text\n",
    "\n",
    "        data_dict['translation'].append(temp_dict)\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6312e1d7-8a87-4788-8714-bc4d37a034e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_ready_dataset(dataset: list, source: str, target: str,\n",
    "                                 model_checkpoint: str,\n",
    "                                 tokenizer: AutoTokenizer):\n",
    "    \"\"\"Makes the data training ready for the model\"\"\"\n",
    "\n",
    "    preped_data = []\n",
    "\n",
    "    for row in dataset:\n",
    "        inputs = (PREFIX + row[source][0])\n",
    "        targets = (row[target][0])\n",
    "\n",
    "        model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH,\n",
    "                                 truncation=True, padding=True)\n",
    "\n",
    "        model_inputs['translation'] = row\n",
    "\n",
    "        # setup the tokenizer for targets\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=MAX_INPUT_LENGTH,\n",
    "                                 truncation=True, padding=True)\n",
    "            model_inputs[LABELS] = labels[INPUT_IDS]\n",
    "\n",
    "        preped_data.append(model_inputs)\n",
    "\n",
    "    return preped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0b2c83b-7c72-4ee4-93fb-428f24f63038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds: tuple) -> dict:\n",
    "    \"\"\"computes bleu score and other performance metrics \"\"\"\n",
    "\n",
    "    metric = load_metric(\"sacrebleu\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {'bleu': result['score']}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "\n",
    "    result['gen_len'] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39d88b-f508-4998-b303-a0942c3fad24",
   "metadata": {},
   "source": [
    "Perform train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f73ec1-e18b-4e05-901e-4a1db57e441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_ZH\n",
    "y = df_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cee608bf-2397-4a43-8449-d2630d0a3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, shuffle = True, random_state = 100)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.20, shuffle = True, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe1b77-6602-441f-a86b-8ddec9ff98c2",
   "metadata": {},
   "source": [
    "Load tokenizer. Using Hensinki-NLP Chinese to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78a4d2d6-0520-40be-87ed-4991a3ae085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96a20c-1fe3-4d39-b915-9d8d03355635",
   "metadata": {},
   "source": [
    "Process dataset to make it suitable for model, including tokenisation and converting it into a Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6b8faeb-f1d2-4877-a140-43ef29932d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training, validation, and test data\n",
    "training_data = prep_data_for_model_fine_tuning(X_train.values, y_train.values)\n",
    "validating_data = prep_data_for_model_fine_tuning(X_val.values, y_val.values)\n",
    "testing_data = prep_data_for_model_fine_tuning(X_test.values, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42def1fc-4c8a-47d1-8b44-d5c01e21bd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keejuan/Documents/ML/CNENTranslation/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Generate model-ready inputs for training, validation, and test\n",
    "train_data = generate_model_ready_dataset(dataset=training_data['translation'],\n",
    "                                          tokenizer=tokenizer,\n",
    "                                          source=CHINESE,\n",
    "                                          target=ENGLISH,\n",
    "                                          model_checkpoint='Helsinki-NLP/opus-mt-zh-en')\n",
    "validation_data = generate_model_ready_dataset(dataset=validating_data['translation'],\n",
    "                                               tokenizer=tokenizer,\n",
    "                                               source=CHINESE,\n",
    "                                               target=ENGLISH,\n",
    "                                               model_checkpoint='Helsinki-NLP/opus-mt-zh-en')\n",
    "test_data = generate_model_ready_dataset(dataset=testing_data['translation'],\n",
    "                                         tokenizer=tokenizer,\n",
    "                                         source=CHINESE,\n",
    "                                         target=ENGLISH,\n",
    "                                         model_checkpoint='Helsinki-NLP/opus-mt-zh-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5fd9c81-25dd-4482-9519-e8b143c695bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "train_df = pd.DataFrame.from_records(train_data)\n",
    "validation_df = pd.DataFrame.from_records(validation_data)\n",
    "test_df = pd.DataFrame.from_records(test_data)\n",
    "\n",
    "# Convert DataFrames to Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde4853-3dd8-4494-adc5-40aca6b4618a",
   "metadata": {},
   "source": [
    "Prepare model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c01f78b-b5d9-492a-86da-01686ea788d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model and define training arguments\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "\n",
    "model_args = Seq2SeqTrainingArguments(\n",
    "    \"translation-pt-en-t5-finetuned-zh-to-en\",\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=0.02,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=7,\n",
    "    predict_with_generate=True\n",
    ")\n",
    "\n",
    "# Create a data collator for sequence-to-sequence tasks\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2417a874-2387-447e-af7f-2716dfef80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d592108-4bf7-494f-98b6-6a154d1bd66b",
   "metadata": {},
   "source": [
    "Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "885dd25c-cb9c-4fba-b7a1-67dd5d4d385f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keejuan/Documents/ML/CNENTranslation/lib/python3.12/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    model_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f55a095d-9319-44b6-b879-dff26eae6da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3150' max='3150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3150/3150 2:53:34, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Blue</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.980045</td>\n",
       "      <td>21.802200</td>\n",
       "      <td>28.609400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.974800</td>\n",
       "      <td>2.019087</td>\n",
       "      <td>21.907500</td>\n",
       "      <td>29.283300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.218500</td>\n",
       "      <td>2.128166</td>\n",
       "      <td>22.105300</td>\n",
       "      <td>28.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.743300</td>\n",
       "      <td>2.228374</td>\n",
       "      <td>22.449400</td>\n",
       "      <td>28.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.440600</td>\n",
       "      <td>2.326017</td>\n",
       "      <td>22.683000</td>\n",
       "      <td>28.571700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.248400</td>\n",
       "      <td>2.400083</td>\n",
       "      <td>22.699100</td>\n",
       "      <td>28.856100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.139100</td>\n",
       "      <td>2.444219</td>\n",
       "      <td>22.971000</td>\n",
       "      <td>29.079400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/pbv_qcy127g2gt_nwhfbrnxr0000gn/T/ipykernel_93670/1112429398.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "/Users/keejuan/Documents/ML/CNENTranslation/lib/python3.12/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "/Users/keejuan/Documents/ML/CNENTranslation/lib/python3.12/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "/Users/keejuan/Documents/ML/CNENTranslation/lib/python3.12/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "/Users/keejuan/Documents/ML/CNENTranslation/lib/python3.12/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "/Users/keejuan/Documents/ML/CNENTranslation/lib/python3.12/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "/Users/keejuan/Documents/ML/CNENTranslation/lib/python3.12/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n",
      "/Users/keejuan/Documents/ML/CNENTranslation/lib/python3.12/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3150, training_loss=0.7614415686471122, metrics={'train_runtime': 10415.4698, 'train_samples_per_second': 4.839, 'train_steps_per_second': 0.302, 'total_flos': 1238519135600640.0, 'train_loss': 0.7614415686471122, 'epoch': 7.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d60520a5-0cae-4612-8aea-21ca03259dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]], 'forced_eos_token_id': 0}\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"FineTunedTranslationModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f67c0-259a-4bce-bd50-b1eda39da67a",
   "metadata": {},
   "source": [
    "Test model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0938c0ed-07ff-4f20-8f4c-58bb64352167",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "828b7c6c-ac37-4541-9025-23a686c6ef2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keejuan/Documents/ML/CNENTranslation/lib/python3.12/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a2f760d-46f3-4bd7-9c45-809a9f65dd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Bleu Score:  22.7497\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Bleu Score: \", test_results.metrics[\"test_blue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f5b440-ac3b-45d2-bc61-db4599ba3475",
   "metadata": {},
   "source": [
    "Generate predicted sentences of test dataset for human evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee9781be-de73-44ff-b84e-7655ab12f2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5720f43a-4ebf-46d3-a2dc-e7c4b3d0e0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [18:39<00:00,  1.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "test_input = test_dataset['translation']\n",
    "\n",
    "for input_text in tqdm(test_input):\n",
    "    source_sentence = input_text[CHINESE]\n",
    "    encoded_source = tokenizer(source_sentence,\n",
    "                               return_tensors='pt',\n",
    "                               padding=True,\n",
    "                               truncation=True)\n",
    "    encoded_source.to(device)  # Move input tensor to the same device as the model\n",
    "\n",
    "    translated = model.generate(**encoded_source)\n",
    "\n",
    "    predictions.append([tokenizer.decode(t, skip_special_tokens=True) for t in translated][0])\n",
    "\n",
    "# Move the model back to CPU if needed\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fbcb743f-78ae-4597-8279-1acd71348fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 143493.12it/s]\n"
     ]
    }
   ],
   "source": [
    "y_true_en = []\n",
    "y_true_pt = []\n",
    "\n",
    "for input_text in tqdm(test_input):\n",
    "    y_true_pt.append(input_text[CHINESE])\n",
    "    y_true_en.append(input_text[ENGLISH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d2cf24e-413d-4a5a-9937-378a4298636d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true_port</th>\n",
       "      <th>y_true_eng</th>\n",
       "      <th>predicted_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[这些 特殊 利益 包括 华尔街 、 大 石油 公司 、 大 人寿保险 商 以及 军火商 。...</td>\n",
       "      <td>[These special interests included Wall Street,...</td>\n",
       "      <td>Such special interests include Wall Street, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[经过 多年 的 争论 和 布什 政府 反对 采取任何 行动 的 态度 影响 ， 美国 终于...</td>\n",
       "      <td>[After years of debate in the United States, a...</td>\n",
       "      <td>After years of wrangling and the Bush administ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[有 数百万 东南亚 人 冒险 到 中东 工作 ， 但是 还有 数百万 人 跨境 到 该 地...</td>\n",
       "      <td>[Millions of people in Southeast Asia venture ...</td>\n",
       "      <td>Many millions of Southeast Asians risk working...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[值得一提的是 三位 前 拉美 总统 ， 墨西哥 排名 第二 的 富翁 里卡多 · 萨利纳斯...</td>\n",
       "      <td>[Three former Latin American presidents, Mexic...</td>\n",
       "      <td>Mention should be taken of three former Latin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[肥料 和 杀虫剂 的 价格 都 比 我 想象 的 要 贵 。  ]</td>\n",
       "      <td>[Fertilizer and spraying were both more expens...</td>\n",
       "      <td>The price of fertilizer and pesticides is more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[解决问题 需要 呼吁 增加 税收 ， 而 乔治 · W · 布什 1992 年 得到 的 ...</td>\n",
       "      <td>[To do so would require calling for higher tax...</td>\n",
       "      <td>Solving the problem requires an increase in ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[美国 的 战略 真空  ]</td>\n",
       "      <td>[America’s Strategy Vacuum ]</td>\n",
       "      <td>America’s Strategy Gap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[根据 对 7000 家 公司 所 做 的 月度 调查 ， Ifo 的 德国 经济 景气 指...</td>\n",
       "      <td>[The Ifo climate indicator for Germany, based ...</td>\n",
       "      <td>According to a monthly survey of 700,000 compa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[其一 是 移民 改革 ， 这个 问题 在 美国 与 在 欧洲 一样 存在 争议 。  ]</td>\n",
       "      <td>[One is immigration reform, which is as contro...</td>\n",
       "      <td>One is immigration reform, a matter that is as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[这种 特性 使得 超导体 拥有 了 可供 多方面 利用 的 独有 特征 。  ]</td>\n",
       "      <td>[Because of this property, superconductors hav...</td>\n",
       "      <td>Such properties put superconductors in the uni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           y_true_port  \\\n",
       "0    [这些 特殊 利益 包括 华尔街 、 大 石油 公司 、 大 人寿保险 商 以及 军火商 。...   \n",
       "1    [经过 多年 的 争论 和 布什 政府 反对 采取任何 行动 的 态度 影响 ， 美国 终于...   \n",
       "2    [有 数百万 东南亚 人 冒险 到 中东 工作 ， 但是 还有 数百万 人 跨境 到 该 地...   \n",
       "3    [值得一提的是 三位 前 拉美 总统 ， 墨西哥 排名 第二 的 富翁 里卡多 · 萨利纳斯...   \n",
       "4                   [肥料 和 杀虫剂 的 价格 都 比 我 想象 的 要 贵 。  ]   \n",
       "..                                                 ...   \n",
       "995  [解决问题 需要 呼吁 增加 税收 ， 而 乔治 · W · 布什 1992 年 得到 的 ...   \n",
       "996                                     [美国 的 战略 真空  ]   \n",
       "997  [根据 对 7000 家 公司 所 做 的 月度 调查 ， Ifo 的 德国 经济 景气 指...   \n",
       "998      [其一 是 移民 改革 ， 这个 问题 在 美国 与 在 欧洲 一样 存在 争议 。  ]   \n",
       "999          [这种 特性 使得 超导体 拥有 了 可供 多方面 利用 的 独有 特征 。  ]   \n",
       "\n",
       "                                            y_true_eng  \\\n",
       "0    [These special interests included Wall Street,...   \n",
       "1    [After years of debate in the United States, a...   \n",
       "2    [Millions of people in Southeast Asia venture ...   \n",
       "3    [Three former Latin American presidents, Mexic...   \n",
       "4    [Fertilizer and spraying were both more expens...   \n",
       "..                                                 ...   \n",
       "995  [To do so would require calling for higher tax...   \n",
       "996                       [America’s Strategy Vacuum ]   \n",
       "997  [The Ifo climate indicator for Germany, based ...   \n",
       "998  [One is immigration reform, which is as contro...   \n",
       "999  [Because of this property, superconductors hav...   \n",
       "\n",
       "                                        predicted_text  \n",
       "0    Such special interests include Wall Street, la...  \n",
       "1    After years of wrangling and the Bush administ...  \n",
       "2    Many millions of Southeast Asians risk working...  \n",
       "3    Mention should be taken of three former Latin ...  \n",
       "4    The price of fertilizer and pesticides is more...  \n",
       "..                                                 ...  \n",
       "995  Solving the problem requires an increase in ta...  \n",
       "996                             America’s Strategy Gap  \n",
       "997  According to a monthly survey of 700,000 compa...  \n",
       "998  One is immigration reform, a matter that is as...  \n",
       "999  Such properties put superconductors in the uni...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = pd.DataFrame({\"y_true_port\": y_true_pt, \"y_true_eng\": y_true_en, \"predicted_text\": predictions})\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ace793dc-58a1-4354-9e39-a2ab8c612c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('translation.csv', index=False, encoding='utf_8_sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
